{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:21:07.869489400Z",
     "start_time": "2024-05-14T13:21:07.805703Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding, multiply, LeakyReLU, ReLU, Softmax\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "# enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:21:07.892412900Z",
     "start_time": "2024-05-14T13:21:07.869489400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.7.0'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:21:07.913342700Z",
     "start_time": "2024-05-14T13:21:07.895402800Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12492/1555151137.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mgpus\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlist_physical_devices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'GPU'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_visible_devices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgpus\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'GPU'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mlogical_gpus\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlist_logical_devices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'GPU'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Physical GPUs:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgpus\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(\"Physical GPUs:\", len(gpus))\n",
    "    print(\"Logical GPUs:\", len(logical_gpus))\n",
    "\n",
    "except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:31:39.442973800Z",
     "start_time": "2024-06-18T16:31:39.337974600Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_17840/2603425539.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mx_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/preserve50/x_train.npy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0my_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/preserve50/y_train.npy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mx_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/preserve50/x_test.npy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0my_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../data/preserve50/y_test.npy\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"../data/preserve50/x_train.npy\")\n",
    "y_train = np.load(\"../data/preserve50/y_train.npy\")\n",
    "x_test = np.load(\"../data/preserve50/x_test.npy\")\n",
    "y_test = np.load(\"../data/preserve50/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:21:08.077792800Z",
     "start_time": "2024-05-14T13:21:08.059853Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        alpha = tf.random.uniform((self.batch_size, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "    \n",
    "class ECGAN():\n",
    "    def __init__(self, \n",
    "                 x_train, \n",
    "                 y_train, \n",
    "                 num_classes: int, \n",
    "                 latent_dim: int, \n",
    "                 batch_size: int,\n",
    "                 n_critic: int,\n",
    "                 conf_thresh: float,\n",
    "                 adv_weight: float):\n",
    "        \"\"\"Implement EC-GAN with an WCGAN-GP and MLP.        \n",
    "        \n",
    "        Attributes\n",
    "        ---------\n",
    "        x_train : numpy.ndarray\n",
    "            Real data without labels used for training.\n",
    "            (Created with sklearn.model_selection.train_test_split\n",
    "        \n",
    "        y_train : numpy.ndarray\n",
    "            Real data labels.\n",
    "            \n",
    "        num_classes : int\n",
    "            Number of data classes. Number of unique elements in y_train.\n",
    "            \n",
    "        data_dim : int\n",
    "            Data dimension. Number of columns in x_train.\n",
    "            \n",
    "        latent_dim : int\n",
    "            Dimension of random noise vector (z), used for training\n",
    "            the generator.\n",
    "            \n",
    "        batch_size : int\n",
    "            Size of training batch in each epoch.\n",
    "        \n",
    "        n_critic : int\n",
    "            Number of times the critic (discriminator) will be trained\n",
    "            in each epoch.\n",
    "            \n",
    "        conf_thresh : float\n",
    "            Confidence threshold. EC-GAN parameter which decides how good\n",
    "            the generated sample needs to be, for it to be fed to the \n",
    "            classifier.\n",
    "        \n",
    "        adv_weight : float\n",
    "            Adverserial weight. EC-GAN parameter which represents the \n",
    "            importance fake data has on classifier training.\n",
    "            Value has been taken from the original paper.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.x_train = x_train.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        \n",
    "        # Store labels as one-hot vectors.\n",
    "        self.y_train_onehot = to_categorical(y_train)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.data_dim = x_train.shape[1]\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # WCGAN-GP parameters. \n",
    "        self.n_critic = n_critic\n",
    "        \n",
    "        # EC-GAN parameters.\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.adv_weight = adv_weight\n",
    "        \n",
    "        # Log training progress.\n",
    "        self.losslog = []\n",
    "        self.class_acc_log = []\n",
    "        self.class_loss_log = []\n",
    "\n",
    "        # Adam optimizer for WCGAN-GP, suggested by original paper.\n",
    "        optimizer = Adam(learning_rate=0.0005, beta_1=0.05, beta_2=0.9)\n",
    "\n",
    "        # Categorical crossentropy loss function for the classifier.\n",
    "        self.cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        # Build the generator, critic and classifier\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "        self.classifier = self.build_classifier()\n",
    "\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic.\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Data input (real sample).\n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        # Noise input (z).\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        # Label input.\n",
    "        label = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        # Generate data based of noise (fake sample)\n",
    "        fake_data = self.generator([noise, label])\n",
    "        \n",
    "        \n",
    "        # Critic (discriminator) determines validity of the real and fake images.\n",
    "        fake = self.critic([fake_data, label])\n",
    "        valid = self.critic([real_data, label])\n",
    "        \n",
    "        # Construct weighted average between real and fake images.\n",
    "        interpolated_data = RandomWeightedAverage(self.batch_size)([real_data, fake_data])\n",
    "        \n",
    "        # Determine validity of weighted sample.\n",
    "        validity_interpolated = self.critic([interpolated_data, label])\n",
    "        \n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument.\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_data)\n",
    "        # Keras requires function names.\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' \n",
    "        \n",
    "        self.critic_model = Model(\n",
    "            inputs=[real_data, label, noise],\n",
    "            outputs=[valid, fake, validity_interpolated]\n",
    "        )\n",
    "        \n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                        self.wasserstein_loss,\n",
    "                                        partial_gp_loss],\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_weights=[1, 1, 10])\n",
    " \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze other's layers.\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator.\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        \n",
    "        # Add label to input.\n",
    "        label = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        # Generate data based of noise.\n",
    "        fake_data = self.generator([noise, label])\n",
    "\n",
    "        # Discriminator determines validity.\n",
    "        valid = self.critic([fake_data, label])\n",
    "\n",
    "        # Defines generator model.\n",
    "        self.generator_model = Model([noise, label], valid)\n",
    "        \n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, \n",
    "                                     optimizer=optimizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #   for the Classifier (real)\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Real data classifier training\n",
    "        \n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        \n",
    "        real_predictions = self.classifier(real_data)\n",
    "        \n",
    "        self.real_classifier_model = Model(real_data, real_predictions)\n",
    "        \n",
    "        self.real_classifier_model.compile(loss=\"categorical_crossentropy\",\n",
    "                                           optimizer=\"adamax\",\n",
    "                                           metrics=[\"accuracy\"])\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #   for the Classifier (fake)\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Fake data classifier training\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        fake_labels = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        \n",
    "        fake_data = self.generator([noise, fake_labels])\n",
    "        \n",
    "        fake_predictions = self.classifier(fake_data)\n",
    "        \n",
    "        self.fake_classifier_model = Model([noise, fake_labels], fake_predictions)\n",
    "        \n",
    "        self.fake_classifier_model.compile(loss=self.ecgan_loss, \n",
    "                                           optimizer=\"adamax\",\n",
    "                                           metrics=[\"accuracy\"])\n",
    "\n",
    "        \n",
    "        \n",
    "    def ecgan_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate loss for fake data predictions.\"\"\"\n",
    "        \n",
    "        max_values = tf.math.reduce_max(y_pred, axis=1)\n",
    "        \n",
    "        max_index = tf.where(tf.math.greater(max_values, self.conf_thresh))\n",
    "        \n",
    "        loss = self.adv_weight * self.cce_loss(y_true[max_index], y_pred[max_index])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential(name=\"Generator\")\n",
    "        \n",
    "        # First hidden layer.\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Second hidden layer.\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Third hidden layer.\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Output layer.\n",
    "        model.add(Dense(self.data_dim, activation=\"tanh\"))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # Noise and label input layers.\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        label = Input(shape=(1,), dtype=\"int32\", name=\"Label\")\n",
    "        \n",
    "        # Embed labels into onehot encoded vectors.\n",
    "        label_embedding = Flatten(name=\"Flatten\")(Embedding(self.num_classes, self.latent_dim, name=\"Embedding\")(label))\n",
    "        \n",
    "        # Multiply noise and embedded labels to be used as model input.\n",
    "        model_input = multiply([noise, label_embedding], name=\"Multiply\")\n",
    "        \n",
    "        generated_data = model(model_input)\n",
    "\n",
    "        return Model(inputs=[noise, label], \n",
    "                     outputs=generated_data, \n",
    "                     name=\"Generator\")\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential(name=\"Critic\")\n",
    "\n",
    "        # First hidden layer.\n",
    "        model.add(Dense(1024, input_dim=self.data_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Second hidden layer.        \n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Third hidden layer.\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        # Output layer with linear activation.\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # Artificial data input.\n",
    "        generated_sample = Input(shape=self.data_dim, name=\"Generated_data\")\n",
    "        # Label input.\n",
    "        label = Input(shape=(1,), dtype=\"int32\", name=\"Label\") \n",
    "        \n",
    "        # Embedd label as onehot vector.\n",
    "        label_embedding = Flatten(name=\"Flatten\")(Embedding(self.num_classes, self.data_dim, name=\"Embedding\")(label))\n",
    "        \n",
    "        # Multiply fake data sample with label embedding to get critic input.\n",
    "        model_input = multiply([generated_sample, label_embedding], name=\"Multiply\")\n",
    "        \n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[generated_sample, label], \n",
    "                     outputs=validity, \n",
    "                     name=\"Critic\")\n",
    "    \n",
    "    def build_classifier(self):\n",
    "        \n",
    "        model = Sequential(name=\"Classifier\")\n",
    "        \n",
    "        # First hidden layer.\n",
    "        model.add(Dense(128, input_dim=self.data_dim))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Second hidden layer.\n",
    "        model.add(Dense(256))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(128))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Output layer.\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Softmax())\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # Data input.\n",
    "        data = Input(shape=self.data_dim, name=\"Data\")\n",
    "\n",
    "        # CLassifier outout is class predictions vector.\n",
    "        predictions = model(data)\n",
    "        \n",
    "        return Model(inputs=data,\n",
    "                     outputs=predictions,\n",
    "                     name=\"Classifier\")\n",
    "        \n",
    "    \n",
    "    def save_models(self, folder_path):\n",
    "        \"\"\"\n",
    "        Save the generator, critic, and classifier models.\n",
    "        \n",
    "        Parameters:\n",
    "            folder_path (str): Folder path to save the models.\n",
    "        \"\"\"\n",
    "        # Create folder if it does not exist.\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        # Save generator model.\n",
    "        generator_path = os.path.join(folder_path, 'generator_model.h5')\n",
    "        self.generator.save(generator_path)\n",
    "        \n",
    "        # Save critic model.\n",
    "        critic_path = os.path.join(folder_path, 'critic_model.h5')\n",
    "        self.critic.save(critic_path)\n",
    "        \n",
    "        # Save classifier model.\n",
    "        classifier_path = os.path.join(folder_path, 'classifier_model.h5')\n",
    "        self.classifier.save(classifier_path)\n",
    "        \n",
    "        print(\"Models saved successfully.\")\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Adversarial ground truths.\n",
    "        valid = -(np.ones((self.batch_size, 1)))\n",
    "        fake =  np.ones((self.batch_size, 1))\n",
    "        dummy = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        # Number of batches.\n",
    "        self.n_batches = math.floor(self.x_train.shape[0] / self.batch_size)\n",
    "\n",
    "        overhead = self.x_train.shape[0] % self.batch_size\n",
    "         \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Reset training set.\n",
    "            self.x_train = x_train.copy()\n",
    "            self.y_train = y_train.copy()\n",
    "\n",
    "            # Select random overhead rows that do not fit into batches.\n",
    "            rand_overhead_idx = np.random.choice(range(self.x_train.shape[0]), overhead, replace=False)\n",
    "\n",
    "            # Remove random overhead rows.\n",
    "            self.x_train = np.delete(self.x_train, rand_overhead_idx, axis=0)\n",
    "            self.y_train = np.delete(self.y_train, rand_overhead_idx, axis=0)\n",
    "\n",
    "\n",
    "            # Split training data into batches.\n",
    "            x_batches = np.split(self.x_train, self.n_batches)\n",
    "            y_batches = np.split(self.y_train, self.n_batches)\n",
    "            \n",
    "            for x_batch, y_batch, i in zip(x_batches, y_batches, range(self.n_batches)):   \n",
    "                \n",
    "                if epoch < 5:\n",
    "                    \n",
    "                    for _ in range(self.n_critic):\n",
    "\n",
    "                        # ---------------------\n",
    "                        #  Train Critic\n",
    "                        # ---------------------\n",
    "\n",
    "                        # Generate random noise.\n",
    "                        noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "                        # Train the critic.\n",
    "                        d_loss = self.critic_model.train_on_batch(\n",
    "                            [x_batch, y_batch, noise],                                      \n",
    "                            [valid, fake, dummy])\n",
    "\n",
    "\n",
    "                    # ---------------------\n",
    "                    #  Train Generator\n",
    "                    # ---------------------\n",
    "\n",
    "                    # Generate sample of artificial labels.\n",
    "                    generated_labels = np.random.randint(1, self.num_classes, self.batch_size).reshape(-1, 1)\n",
    "\n",
    "                    # Train generator.\n",
    "                    \n",
    "                    g_loss = self.generator_model.train_on_batch([noise, generated_labels], valid)\n",
    "\n",
    "\n",
    "                    # ---------------------\n",
    "                    #  Train Classifier\n",
    "                    # ---------------------\n",
    "\n",
    "                    # One-hot encode real labels.\n",
    "                    y_batch = to_categorical(y_batch, self.num_classes)\n",
    "\n",
    "                    # One-hot encode generated labels.\n",
    "                    generated_labels_onehot = to_categorical(generated_labels, self.num_classes)\n",
    "                    \n",
    "                    real_loss = self.real_classifier_model.train_on_batch(x_batch, y_batch)\n",
    "                    \n",
    "                    fake_loss = self.fake_classifier_model.train_on_batch([noise, generated_labels], generated_labels_onehot)\n",
    "                    \n",
    "                    # Classifier loss as presented in EC-GAN paper.\n",
    "                    c_loss = (real_loss[0] + fake_loss[0]) / (1 + self.adv_weight)\n",
    "\n",
    "                    avg_acc = np.mean([real_loss[1], fake_loss[1]])\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # ---------------------\n",
    "                    #  Train Classifier\n",
    "                    # ---------------------\n",
    "                    \n",
    "                    # Generate random noise.\n",
    "                    noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "                    # Generate sample of artificial labels.\n",
    "                    generated_labels = np.random.randint(1, self.num_classes, self.batch_size).reshape(-1, 1)\n",
    "\n",
    "                    # One-hot encode real labels.\n",
    "                    y_batch = to_categorical(y_batch, self.num_classes)\n",
    "\n",
    "                    # One-hot encode generated labels.\n",
    "                    generated_labels_onehot = to_categorical(generated_labels, self.num_classes)\n",
    "\n",
    "                    real_loss = self.real_classifier_model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "                    fake_loss = self.fake_classifier_model.train_on_batch([noise, generated_labels], generated_labels_onehot)\n",
    "\n",
    "                    # Classifier loss as presented in EC-GAN paper.\n",
    "                    c_loss = (real_loss[0] + fake_loss[0]) / (1 + self.adv_weight)\n",
    "\n",
    "                    avg_acc = np.mean([real_loss[1], fake_loss[1]])\n",
    "\n",
    "\n",
    "                # ---------------------\n",
    "                #  Logging\n",
    "                # ---------------------\n",
    "\n",
    "                self.losslog.append([d_loss[0], g_loss, c_loss])\n",
    "                self.class_loss_log.append([real_loss[0], fake_loss[0], c_loss])\n",
    "                self.class_acc_log.append([real_loss[1], fake_loss[1], avg_acc])\n",
    "\n",
    "                # Plot progress.\n",
    "                DLOSS = \"%.4f\" % d_loss[0]\n",
    "                GLOSS = \"%.4f\" % g_loss\n",
    "                CLOSS = \"%.4f\" % c_loss\n",
    "                RLOSS = \"%.4f\" % real_loss[0]\n",
    "                FLOSS = \"%.4f\" % fake_loss[0]\n",
    "                CACC  = \"%.4f\" % real_loss[1]\n",
    "                \n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print (f\"{epoch} - {i}/{self.n_batches} \\t [D loss: {DLOSS}] [G loss: {GLOSS}] [R loss: {RLOSS} | F loss: {FLOSS} | C loss: {CLOSS} - C acc: {CACC}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:51:12.923223Z",
     "start_time": "2024-05-14T13:21:08.076796100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 256)               8448      \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1024)              525312    \n",
      "                                                                 \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 31)                31775     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 697,119\n",
      "Trainable params: 697,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Critic\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 1024)              32768     \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 689,153\n",
      "Trainable params: 689,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 128)               4096      \n",
      "                                                                 \n",
      " re_lu_9 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " re_lu_10 (ReLU)             (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 15)                1935      \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         (None, 15)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,951\n",
      "Trainable params: 71,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 - 0/8297 \t [D loss: 8.4719] [G loss: 0.0341] [R loss: 2.5828 | F loss: 0.0000 | C loss: 2.3480 - C acc: 0.5000]\n",
      "0 - 100/8297 \t [D loss: -1.3169] [G loss: -0.3938] [R loss: 0.5436 | F loss: 0.2730 | C loss: 0.7424 - C acc: 0.8047]\n",
      "0 - 200/8297 \t [D loss: -0.9469] [G loss: -0.5007] [R loss: 0.3776 | F loss: 0.2264 | C loss: 0.5492 - C acc: 0.9141]\n",
      "0 - 300/8297 \t [D loss: -0.8186] [G loss: -0.4606] [R loss: 0.3378 | F loss: 0.2154 | C loss: 0.5030 - C acc: 0.8828]\n",
      "0 - 400/8297 \t [D loss: -0.7327] [G loss: -0.2439] [R loss: 0.2822 | F loss: 0.1917 | C loss: 0.4308 - C acc: 0.9297]\n",
      "0 - 500/8297 \t [D loss: -0.5648] [G loss: -0.0113] [R loss: 0.3337 | F loss: 0.2670 | C loss: 0.5461 - C acc: 0.8984]\n",
      "0 - 600/8297 \t [D loss: -0.5994] [G loss: -0.0449] [R loss: 0.3341 | F loss: 0.2745 | C loss: 0.5532 - C acc: 0.8984]\n",
      "0 - 700/8297 \t [D loss: -0.4429] [G loss: 0.2386] [R loss: 0.4137 | F loss: 0.2589 | C loss: 0.6114 - C acc: 0.8906]\n",
      "0 - 800/8297 \t [D loss: -0.5114] [G loss: 0.1697] [R loss: 0.4408 | F loss: 0.3555 | C loss: 0.7239 - C acc: 0.8750]\n",
      "0 - 900/8297 \t [D loss: -0.5074] [G loss: 0.0567] [R loss: 0.3124 | F loss: 0.2772 | C loss: 0.5360 - C acc: 0.9375]\n",
      "0 - 1000/8297 \t [D loss: -0.5298] [G loss: -0.0316] [R loss: 0.3723 | F loss: 0.2974 | C loss: 0.6088 - C acc: 0.9375]\n",
      "0 - 1100/8297 \t [D loss: -0.4588] [G loss: -0.0938] [R loss: 0.4808 | F loss: 0.2664 | C loss: 0.6793 - C acc: 0.8906]\n",
      "0 - 1200/8297 \t [D loss: -0.5144] [G loss: 0.0461] [R loss: 0.3004 | F loss: 0.3083 | C loss: 0.5534 - C acc: 0.9219]\n",
      "0 - 1300/8297 \t [D loss: -0.5047] [G loss: 0.1837] [R loss: 0.2835 | F loss: 0.2726 | C loss: 0.5055 - C acc: 0.9453]\n",
      "0 - 1400/8297 \t [D loss: -0.4941] [G loss: 0.0493] [R loss: 0.3254 | F loss: 0.1977 | C loss: 0.4755 - C acc: 0.9297]\n",
      "0 - 1500/8297 \t [D loss: -0.4890] [G loss: -0.0042] [R loss: 0.3662 | F loss: 0.2330 | C loss: 0.5447 - C acc: 0.9219]\n",
      "0 - 1600/8297 \t [D loss: -0.4458] [G loss: 0.2339] [R loss: 0.3588 | F loss: 0.2302 | C loss: 0.5355 - C acc: 0.8984]\n",
      "0 - 1700/8297 \t [D loss: -0.4767] [G loss: 0.2162] [R loss: 0.2491 | F loss: 0.2524 | C loss: 0.4559 - C acc: 0.9688]\n",
      "0 - 1800/8297 \t [D loss: -0.4599] [G loss: 0.0851] [R loss: 0.2509 | F loss: 0.2871 | C loss: 0.4890 - C acc: 0.9531]\n",
      "0 - 1900/8297 \t [D loss: -0.4937] [G loss: 0.3930] [R loss: 0.2821 | F loss: 0.2346 | C loss: 0.4698 - C acc: 0.9219]\n",
      "0 - 2000/8297 \t [D loss: -0.4718] [G loss: 0.1744] [R loss: 0.4302 | F loss: 0.2410 | C loss: 0.6102 - C acc: 0.9062]\n",
      "0 - 2100/8297 \t [D loss: -0.4578] [G loss: 0.3279] [R loss: 0.2903 | F loss: 0.2203 | C loss: 0.4642 - C acc: 0.9453]\n",
      "0 - 2200/8297 \t [D loss: -0.4224] [G loss: 0.3616] [R loss: 0.3422 | F loss: 0.1821 | C loss: 0.4766 - C acc: 0.9141]\n",
      "0 - 2300/8297 \t [D loss: -0.4162] [G loss: 0.4252] [R loss: 0.3588 | F loss: 0.2140 | C loss: 0.5207 - C acc: 0.8750]\n",
      "0 - 2400/8297 \t [D loss: -0.4696] [G loss: 0.5351] [R loss: 0.1234 | F loss: 0.1682 | C loss: 0.2651 - C acc: 0.9844]\n",
      "0 - 2500/8297 \t [D loss: -0.4979] [G loss: 0.4271] [R loss: 0.2101 | F loss: 0.1579 | C loss: 0.3345 - C acc: 0.9453]\n",
      "0 - 2600/8297 \t [D loss: -0.4497] [G loss: 0.5491] [R loss: 0.2560 | F loss: 0.1843 | C loss: 0.4003 - C acc: 0.9297]\n",
      "0 - 2700/8297 \t [D loss: -0.4823] [G loss: 0.6990] [R loss: 0.2789 | F loss: 0.1491 | C loss: 0.3890 - C acc: 0.9219]\n",
      "0 - 2800/8297 \t [D loss: -0.4819] [G loss: 0.8031] [R loss: 0.2250 | F loss: 0.1423 | C loss: 0.3340 - C acc: 0.9375]\n",
      "0 - 2900/8297 \t [D loss: -0.5930] [G loss: 0.7236] [R loss: 0.2873 | F loss: 0.1119 | C loss: 0.3629 - C acc: 0.9297]\n",
      "0 - 3000/8297 \t [D loss: -0.5771] [G loss: 1.0211] [R loss: 0.2216 | F loss: 0.1765 | C loss: 0.3620 - C acc: 0.9297]\n",
      "0 - 3100/8297 \t [D loss: -0.5171] [G loss: 0.9161] [R loss: 0.2793 | F loss: 0.0992 | C loss: 0.3441 - C acc: 0.9375]\n",
      "0 - 3200/8297 \t [D loss: -0.5884] [G loss: 0.7842] [R loss: 0.3492 | F loss: 0.1083 | C loss: 0.4159 - C acc: 0.8984]\n",
      "0 - 3300/8297 \t [D loss: -0.5450] [G loss: 0.7149] [R loss: 0.3073 | F loss: 0.1397 | C loss: 0.4064 - C acc: 0.9062]\n",
      "0 - 3400/8297 \t [D loss: -0.5757] [G loss: 0.8419] [R loss: 0.3553 | F loss: 0.1105 | C loss: 0.4235 - C acc: 0.8906]\n",
      "0 - 3500/8297 \t [D loss: -0.5980] [G loss: 0.9147] [R loss: 0.2388 | F loss: 0.1218 | C loss: 0.3278 - C acc: 0.9219]\n",
      "0 - 3600/8297 \t [D loss: -0.6374] [G loss: 0.8255] [R loss: 0.3941 | F loss: 0.0848 | C loss: 0.4353 - C acc: 0.8984]\n",
      "0 - 3700/8297 \t [D loss: -0.5095] [G loss: 0.6711] [R loss: 0.2040 | F loss: 0.0779 | C loss: 0.2563 - C acc: 0.9453]\n",
      "0 - 3800/8297 \t [D loss: -0.5547] [G loss: 0.5703] [R loss: 0.2534 | F loss: 0.0797 | C loss: 0.3029 - C acc: 0.9062]\n",
      "0 - 3900/8297 \t [D loss: -0.5576] [G loss: 0.6474] [R loss: 0.2438 | F loss: 0.0557 | C loss: 0.2722 - C acc: 0.9297]\n",
      "0 - 4000/8297 \t [D loss: -0.5974] [G loss: 0.6264] [R loss: 0.1612 | F loss: 0.0700 | C loss: 0.2102 - C acc: 0.9531]\n",
      "0 - 4100/8297 \t [D loss: -0.5914] [G loss: 0.3881] [R loss: 0.1799 | F loss: 0.0900 | C loss: 0.2453 - C acc: 0.9297]\n",
      "0 - 4200/8297 \t [D loss: -0.5571] [G loss: 0.2268] [R loss: 0.2930 | F loss: 0.0722 | C loss: 0.3320 - C acc: 0.9062]\n",
      "0 - 4300/8297 \t [D loss: -0.5835] [G loss: 0.0369] [R loss: 0.2184 | F loss: 0.0446 | C loss: 0.2391 - C acc: 0.9297]\n",
      "0 - 4400/8297 \t [D loss: -0.6605] [G loss: 0.1842] [R loss: 0.2213 | F loss: 0.0742 | C loss: 0.2687 - C acc: 0.9609]\n",
      "0 - 4500/8297 \t [D loss: -0.6160] [G loss: 0.0804] [R loss: 0.1986 | F loss: 0.0649 | C loss: 0.2395 - C acc: 0.9453]\n",
      "0 - 4600/8297 \t [D loss: -0.5859] [G loss: 0.1313] [R loss: 0.1987 | F loss: 0.0513 | C loss: 0.2273 - C acc: 0.9375]\n",
      "0 - 4700/8297 \t [D loss: -0.5518] [G loss: 0.1322] [R loss: 0.1497 | F loss: 0.0659 | C loss: 0.1960 - C acc: 0.9688]\n",
      "0 - 4800/8297 \t [D loss: -0.5554] [G loss: 0.1300] [R loss: 0.2241 | F loss: 0.0503 | C loss: 0.2495 - C acc: 0.9219]\n",
      "0 - 4900/8297 \t [D loss: -0.5561] [G loss: 0.2965] [R loss: 0.2159 | F loss: 0.0763 | C loss: 0.2656 - C acc: 0.9219]\n",
      "0 - 5000/8297 \t [D loss: -0.5454] [G loss: 0.1828] [R loss: 0.1651 | F loss: 0.0638 | C loss: 0.2081 - C acc: 0.9609]\n",
      "0 - 5100/8297 \t [D loss: -0.5658] [G loss: 0.0717] [R loss: 0.2554 | F loss: 0.0857 | C loss: 0.3101 - C acc: 0.9141]\n",
      "0 - 5200/8297 \t [D loss: -0.5127] [G loss: 0.0701] [R loss: 0.2453 | F loss: 0.0606 | C loss: 0.2781 - C acc: 0.9141]\n",
      "0 - 5300/8297 \t [D loss: -0.4841] [G loss: 0.0316] [R loss: 0.2119 | F loss: 0.0671 | C loss: 0.2537 - C acc: 0.9297]\n",
      "0 - 5400/8297 \t [D loss: -0.5178] [G loss: 0.0943] [R loss: 0.2817 | F loss: 0.0618 | C loss: 0.3122 - C acc: 0.9141]\n",
      "0 - 5500/8297 \t [D loss: -0.5588] [G loss: 0.0850] [R loss: 0.2453 | F loss: 0.0654 | C loss: 0.2824 - C acc: 0.9219]\n",
      "0 - 5600/8297 \t [D loss: -0.5756] [G loss: 0.0263] [R loss: 0.1614 | F loss: 0.0907 | C loss: 0.2291 - C acc: 0.9375]\n",
      "0 - 5700/8297 \t [D loss: -0.6218] [G loss: 0.0979] [R loss: 0.2451 | F loss: 0.0593 | C loss: 0.2767 - C acc: 0.9219]\n",
      "0 - 5800/8297 \t [D loss: -0.5795] [G loss: 0.0751] [R loss: 0.3008 | F loss: 0.0636 | C loss: 0.3313 - C acc: 0.8984]\n",
      "0 - 5900/8297 \t [D loss: -0.5566] [G loss: 0.0496] [R loss: 0.2217 | F loss: 0.0670 | C loss: 0.2624 - C acc: 0.9375]\n",
      "0 - 6000/8297 \t [D loss: -0.5801] [G loss: 0.0775] [R loss: 0.1986 | F loss: 0.0553 | C loss: 0.2308 - C acc: 0.9453]\n",
      "0 - 6100/8297 \t [D loss: -0.5687] [G loss: 0.1424] [R loss: 0.1699 | F loss: 0.0517 | C loss: 0.2014 - C acc: 0.9453]\n",
      "0 - 6200/8297 \t [D loss: -0.4987] [G loss: 0.1760] [R loss: 0.1335 | F loss: 0.0471 | C loss: 0.1642 - C acc: 0.9453]\n",
      "0 - 6300/8297 \t [D loss: -0.5816] [G loss: 0.2208] [R loss: 0.1477 | F loss: 0.0359 | C loss: 0.1669 - C acc: 0.9453]\n",
      "0 - 6400/8297 \t [D loss: -0.5227] [G loss: 0.1059] [R loss: 0.3113 | F loss: 0.0604 | C loss: 0.3380 - C acc: 0.8906]\n",
      "0 - 6500/8297 \t [D loss: -0.5296] [G loss: 0.1360] [R loss: 0.3500 | F loss: 0.0707 | C loss: 0.3825 - C acc: 0.9062]\n",
      "0 - 6600/8297 \t [D loss: -0.5589] [G loss: 0.1312] [R loss: 0.2743 | F loss: 0.0670 | C loss: 0.3103 - C acc: 0.8828]\n",
      "0 - 6700/8297 \t [D loss: -0.5735] [G loss: 0.1217] [R loss: 0.2993 | F loss: 0.0633 | C loss: 0.3296 - C acc: 0.8984]\n",
      "0 - 6800/8297 \t [D loss: -0.5360] [G loss: 0.1406] [R loss: 0.3348 | F loss: 0.0781 | C loss: 0.3754 - C acc: 0.9141]\n",
      "0 - 6900/8297 \t [D loss: -0.5514] [G loss: 0.1518] [R loss: 0.1732 | F loss: 0.0473 | C loss: 0.2005 - C acc: 0.9219]\n",
      "0 - 7000/8297 \t [D loss: -0.5523] [G loss: 0.1399] [R loss: 0.2926 | F loss: 0.0595 | C loss: 0.3200 - C acc: 0.8984]\n",
      "0 - 7100/8297 \t [D loss: -0.5270] [G loss: 0.2520] [R loss: 0.1710 | F loss: 0.0691 | C loss: 0.2183 - C acc: 0.9453]\n",
      "0 - 7200/8297 \t [D loss: -0.5741] [G loss: 0.1547] [R loss: 0.2971 | F loss: 0.1069 | C loss: 0.3673 - C acc: 0.9062]\n",
      "0 - 7300/8297 \t [D loss: -0.5933] [G loss: 0.1579] [R loss: 0.2416 | F loss: 0.0589 | C loss: 0.2731 - C acc: 0.9297]\n",
      "0 - 7400/8297 \t [D loss: -0.5669] [G loss: 0.2080] [R loss: 0.1915 | F loss: 0.0531 | C loss: 0.2223 - C acc: 0.9453]\n",
      "0 - 7500/8297 \t [D loss: -0.5368] [G loss: 0.1906] [R loss: 0.1965 | F loss: 0.0607 | C loss: 0.2338 - C acc: 0.9297]\n",
      "0 - 7600/8297 \t [D loss: -0.6071] [G loss: 0.1024] [R loss: 0.2213 | F loss: 0.0795 | C loss: 0.2734 - C acc: 0.9141]\n",
      "0 - 7700/8297 \t [D loss: -0.5974] [G loss: 0.1283] [R loss: 0.3036 | F loss: 0.0315 | C loss: 0.3047 - C acc: 0.9375]\n",
      "0 - 7800/8297 \t [D loss: -0.5842] [G loss: 0.1494] [R loss: 0.2614 | F loss: 0.0368 | C loss: 0.2710 - C acc: 0.8984]\n",
      "0 - 7900/8297 \t [D loss: -0.5303] [G loss: 0.0875] [R loss: 0.1537 | F loss: 0.0422 | C loss: 0.1781 - C acc: 0.9766]\n",
      "0 - 8000/8297 \t [D loss: -0.5407] [G loss: 0.1708] [R loss: 0.1923 | F loss: 0.0481 | C loss: 0.2185 - C acc: 0.9219]\n",
      "0 - 8100/8297 \t [D loss: -0.5690] [G loss: 0.1589] [R loss: 0.1618 | F loss: 0.0784 | C loss: 0.2184 - C acc: 0.9453]\n",
      "0 - 8200/8297 \t [D loss: -0.5934] [G loss: 0.1341] [R loss: 0.2142 | F loss: 0.0554 | C loss: 0.2451 - C acc: 0.9062]\n",
      "1 - 0/8297 \t [D loss: -0.5755] [G loss: 0.1585] [R loss: 0.1722 | F loss: 0.0386 | C loss: 0.1916 - C acc: 0.9453]\n",
      "1 - 100/8297 \t [D loss: -0.5190] [G loss: 0.1632] [R loss: 0.1385 | F loss: 0.0706 | C loss: 0.1901 - C acc: 0.9375]\n",
      "1 - 200/8297 \t [D loss: -0.5735] [G loss: 0.2195] [R loss: 0.3304 | F loss: 0.0412 | C loss: 0.3378 - C acc: 0.8594]\n",
      "1 - 300/8297 \t [D loss: -0.5638] [G loss: 0.2590] [R loss: 0.2590 | F loss: 0.0410 | C loss: 0.2727 - C acc: 0.8906]\n",
      "1 - 400/8297 \t [D loss: -0.5635] [G loss: 0.2528] [R loss: 0.2294 | F loss: 0.0687 | C loss: 0.2710 - C acc: 0.9141]\n",
      "1 - 500/8297 \t [D loss: -0.5256] [G loss: 0.1980] [R loss: 0.2069 | F loss: 0.0857 | C loss: 0.2660 - C acc: 0.9141]\n",
      "1 - 600/8297 \t [D loss: -0.5454] [G loss: 0.1569] [R loss: 0.2217 | F loss: 0.0666 | C loss: 0.2621 - C acc: 0.9297]\n",
      "1 - 700/8297 \t [D loss: -0.5850] [G loss: 0.2467] [R loss: 0.2009 | F loss: 0.0562 | C loss: 0.2338 - C acc: 0.9219]\n",
      "1 - 800/8297 \t [D loss: -0.5583] [G loss: 0.1303] [R loss: 0.3568 | F loss: 0.0272 | C loss: 0.3491 - C acc: 0.8828]\n",
      "1 - 900/8297 \t [D loss: -0.6080] [G loss: 0.2669] [R loss: 0.2517 | F loss: 0.0670 | C loss: 0.2897 - C acc: 0.9141]\n",
      "1 - 1000/8297 \t [D loss: -0.5653] [G loss: 0.2817] [R loss: 0.2792 | F loss: 0.0457 | C loss: 0.2954 - C acc: 0.9297]\n",
      "1 - 1100/8297 \t [D loss: -0.5515] [G loss: 0.1332] [R loss: 0.2274 | F loss: 0.0336 | C loss: 0.2373 - C acc: 0.9219]\n",
      "1 - 1200/8297 \t [D loss: -0.5903] [G loss: 0.2412] [R loss: 0.2113 | F loss: 0.0740 | C loss: 0.2594 - C acc: 0.9297]\n",
      "1 - 1300/8297 \t [D loss: -0.6008] [G loss: 0.2174] [R loss: 0.1951 | F loss: 0.0532 | C loss: 0.2257 - C acc: 0.9297]\n",
      "1 - 1400/8297 \t [D loss: -0.5397] [G loss: 0.1933] [R loss: 0.1858 | F loss: 0.0535 | C loss: 0.2176 - C acc: 0.9375]\n",
      "1 - 1500/8297 \t [D loss: -0.5205] [G loss: 0.1914] [R loss: 0.2158 | F loss: 0.0478 | C loss: 0.2397 - C acc: 0.9219]\n",
      "1 - 1600/8297 \t [D loss: -0.5887] [G loss: 0.2013] [R loss: 0.3044 | F loss: 0.0739 | C loss: 0.3439 - C acc: 0.8984]\n",
      "1 - 1700/8297 \t [D loss: -0.5568] [G loss: 0.2272] [R loss: 0.1819 | F loss: 0.0449 | C loss: 0.2061 - C acc: 0.9297]\n",
      "1 - 1800/8297 \t [D loss: -0.5401] [G loss: 0.1853] [R loss: 0.1752 | F loss: 0.0569 | C loss: 0.2110 - C acc: 0.9453]\n",
      "1 - 1900/8297 \t [D loss: -0.5615] [G loss: 0.1992] [R loss: 0.2247 | F loss: 0.0518 | C loss: 0.2514 - C acc: 0.9219]\n",
      "1 - 2000/8297 \t [D loss: -0.5348] [G loss: 0.1588] [R loss: 0.2982 | F loss: 0.0813 | C loss: 0.3450 - C acc: 0.9453]\n",
      "1 - 2100/8297 \t [D loss: -0.5502] [G loss: 0.1313] [R loss: 0.3544 | F loss: 0.0623 | C loss: 0.3788 - C acc: 0.8672]\n",
      "1 - 2200/8297 \t [D loss: -0.5489] [G loss: 0.1303] [R loss: 0.2839 | F loss: 0.0496 | C loss: 0.3032 - C acc: 0.9062]\n",
      "1 - 2300/8297 \t [D loss: -0.5915] [G loss: 0.1375] [R loss: 0.2427 | F loss: 0.0550 | C loss: 0.2707 - C acc: 0.9375]\n",
      "1 - 2400/8297 \t [D loss: -0.6292] [G loss: 0.0951] [R loss: 0.2005 | F loss: 0.0623 | C loss: 0.2389 - C acc: 0.9531]\n",
      "1 - 2500/8297 \t [D loss: -0.5723] [G loss: 0.0551] [R loss: 0.2218 | F loss: 0.0535 | C loss: 0.2503 - C acc: 0.9453]\n",
      "1 - 2600/8297 \t [D loss: -0.5717] [G loss: 0.0632] [R loss: 0.2143 | F loss: 0.0745 | C loss: 0.2625 - C acc: 0.9297]\n",
      "1 - 2700/8297 \t [D loss: -0.5254] [G loss: 0.0770] [R loss: 0.2004 | F loss: 0.0423 | C loss: 0.2206 - C acc: 0.9375]\n",
      "1 - 2800/8297 \t [D loss: -0.5300] [G loss: 0.0655] [R loss: 0.2325 | F loss: 0.0516 | C loss: 0.2583 - C acc: 0.9219]\n",
      "1 - 2900/8297 \t [D loss: -0.5817] [G loss: 0.0905] [R loss: 0.3357 | F loss: 0.0381 | C loss: 0.3398 - C acc: 0.8750]\n",
      "1 - 3000/8297 \t [D loss: -0.5746] [G loss: 0.0273] [R loss: 0.3673 | F loss: 0.0451 | C loss: 0.3749 - C acc: 0.8984]\n",
      "1 - 3100/8297 \t [D loss: -0.6259] [G loss: 0.1136] [R loss: 0.2100 | F loss: 0.0571 | C loss: 0.2428 - C acc: 0.9375]\n",
      "1 - 3200/8297 \t [D loss: -0.6009] [G loss: 0.0600] [R loss: 0.2614 | F loss: 0.0312 | C loss: 0.2660 - C acc: 0.9297]\n",
      "1 - 3300/8297 \t [D loss: -0.5648] [G loss: 0.0250] [R loss: 0.2256 | F loss: 0.0525 | C loss: 0.2528 - C acc: 0.9219]\n",
      "1 - 3400/8297 \t [D loss: -0.5652] [G loss: -0.0003] [R loss: 0.2520 | F loss: 0.0275 | C loss: 0.2541 - C acc: 0.9062]\n",
      "1 - 3500/8297 \t [D loss: -0.5793] [G loss: 0.0347] [R loss: 0.2407 | F loss: 0.0487 | C loss: 0.2631 - C acc: 0.8984]\n",
      "1 - 3600/8297 \t [D loss: -0.5766] [G loss: -0.0424] [R loss: 0.2628 | F loss: 0.0537 | C loss: 0.2877 - C acc: 0.9141]\n",
      "1 - 3700/8297 \t [D loss: -0.5786] [G loss: 0.0079] [R loss: 0.2129 | F loss: 0.0771 | C loss: 0.2637 - C acc: 0.9219]\n",
      "1 - 3800/8297 \t [D loss: -0.6268] [G loss: 0.0113] [R loss: 0.2207 | F loss: 0.0491 | C loss: 0.2453 - C acc: 0.9219]\n",
      "1 - 3900/8297 \t [D loss: -0.5841] [G loss: 0.0005] [R loss: 0.2502 | F loss: 0.0406 | C loss: 0.2644 - C acc: 0.8984]\n",
      "1 - 4000/8297 \t [D loss: -0.6283] [G loss: -0.0005] [R loss: 0.1923 | F loss: 0.0509 | C loss: 0.2210 - C acc: 0.9531]\n",
      "1 - 4100/8297 \t [D loss: -0.6297] [G loss: 0.0016] [R loss: 0.2342 | F loss: 0.0314 | C loss: 0.2414 - C acc: 0.9297]\n",
      "1 - 4200/8297 \t [D loss: -0.5783] [G loss: 0.0087] [R loss: 0.2887 | F loss: 0.0502 | C loss: 0.3081 - C acc: 0.9219]\n",
      "1 - 4300/8297 \t [D loss: -0.5653] [G loss: 0.0485] [R loss: 0.1896 | F loss: 0.0313 | C loss: 0.2009 - C acc: 0.9297]\n",
      "1 - 4400/8297 \t [D loss: -0.6570] [G loss: 0.0556] [R loss: 0.1944 | F loss: 0.0463 | C loss: 0.2188 - C acc: 0.9531]\n",
      "1 - 4500/8297 \t [D loss: -0.6317] [G loss: -0.0695] [R loss: 0.1223 | F loss: 0.0302 | C loss: 0.1386 - C acc: 0.9531]\n",
      "1 - 4600/8297 \t [D loss: -0.6113] [G loss: -0.0633] [R loss: 0.2168 | F loss: 0.0487 | C loss: 0.2413 - C acc: 0.9453]\n",
      "1 - 4700/8297 \t [D loss: -0.6037] [G loss: -0.0221] [R loss: 0.2435 | F loss: 0.0325 | C loss: 0.2509 - C acc: 0.9062]\n",
      "1 - 4800/8297 \t [D loss: -0.6063] [G loss: -0.0513] [R loss: 0.2168 | F loss: 0.0546 | C loss: 0.2467 - C acc: 0.9062]\n",
      "1 - 4900/8297 \t [D loss: -0.5939] [G loss: -0.0116] [R loss: 0.1332 | F loss: 0.0288 | C loss: 0.1473 - C acc: 0.9609]\n",
      "1 - 5000/8297 \t [D loss: -0.5801] [G loss: 0.0324] [R loss: 0.1401 | F loss: 0.0297 | C loss: 0.1544 - C acc: 0.9609]\n",
      "1 - 5100/8297 \t [D loss: -0.6059] [G loss: -0.0561] [R loss: 0.2124 | F loss: 0.0574 | C loss: 0.2453 - C acc: 0.9297]\n",
      "1 - 5200/8297 \t [D loss: -0.6225] [G loss: -0.0409] [R loss: 0.2206 | F loss: 0.0233 | C loss: 0.2217 - C acc: 0.9062]\n",
      "1 - 5300/8297 \t [D loss: -0.5777] [G loss: -0.0021] [R loss: 0.2160 | F loss: 0.0399 | C loss: 0.2326 - C acc: 0.9531]\n",
      "1 - 5400/8297 \t [D loss: -0.5789] [G loss: -0.0088] [R loss: 0.3066 | F loss: 0.0389 | C loss: 0.3141 - C acc: 0.8906]\n",
      "1 - 5500/8297 \t [D loss: -0.6235] [G loss: 0.0753] [R loss: 0.2308 | F loss: 0.0468 | C loss: 0.2524 - C acc: 0.9062]\n",
      "1 - 5600/8297 \t [D loss: -0.6121] [G loss: 0.0273] [R loss: 0.1475 | F loss: 0.0207 | C loss: 0.1529 - C acc: 0.9453]\n",
      "1 - 5700/8297 \t [D loss: -0.6472] [G loss: -0.0600] [R loss: 0.2483 | F loss: 0.0532 | C loss: 0.2741 - C acc: 0.9297]\n",
      "1 - 5800/8297 \t [D loss: -0.6227] [G loss: -0.0414] [R loss: 0.2317 | F loss: 0.0631 | C loss: 0.2680 - C acc: 0.9219]\n",
      "1 - 5900/8297 \t [D loss: -0.5784] [G loss: -0.0546] [R loss: 0.2558 | F loss: 0.0494 | C loss: 0.2775 - C acc: 0.9062]\n",
      "1 - 6000/8297 \t [D loss: -0.6240] [G loss: 0.0702] [R loss: 0.2452 | F loss: 0.0467 | C loss: 0.2654 - C acc: 0.9297]\n",
      "1 - 6100/8297 \t [D loss: -0.6373] [G loss: -0.0374] [R loss: 0.1041 | F loss: 0.0565 | C loss: 0.1460 - C acc: 0.9766]\n",
      "1 - 6200/8297 \t [D loss: -0.5953] [G loss: -0.0889] [R loss: 0.1198 | F loss: 0.0456 | C loss: 0.1504 - C acc: 0.9766]\n",
      "1 - 6300/8297 \t [D loss: -0.6266] [G loss: -0.0372] [R loss: 0.1358 | F loss: 0.0512 | C loss: 0.1699 - C acc: 0.9453]\n",
      "1 - 6400/8297 \t [D loss: -0.5690] [G loss: -0.1292] [R loss: 0.4349 | F loss: 0.0415 | C loss: 0.4331 - C acc: 0.8906]\n",
      "1 - 6500/8297 \t [D loss: -0.6538] [G loss: -0.2074] [R loss: 0.2397 | F loss: 0.0393 | C loss: 0.2536 - C acc: 0.9531]\n",
      "1 - 6600/8297 \t [D loss: -0.6120] [G loss: -0.3107] [R loss: 0.1516 | F loss: 0.0358 | C loss: 0.1704 - C acc: 0.9297]\n",
      "1 - 6700/8297 \t [D loss: -0.6363] [G loss: -0.2917] [R loss: 0.2486 | F loss: 0.0377 | C loss: 0.2603 - C acc: 0.8906]\n",
      "1 - 6800/8297 \t [D loss: -0.5850] [G loss: -0.2389] [R loss: 0.3517 | F loss: 0.0591 | C loss: 0.3734 - C acc: 0.8672]\n",
      "1 - 6900/8297 \t [D loss: -0.6046] [G loss: -0.2713] [R loss: 0.1549 | F loss: 0.0313 | C loss: 0.1693 - C acc: 0.9453]\n",
      "1 - 7000/8297 \t [D loss: -0.6113] [G loss: -0.2765] [R loss: 0.2402 | F loss: 0.0412 | C loss: 0.2558 - C acc: 0.9141]\n",
      "1 - 7100/8297 \t [D loss: -0.6872] [G loss: -0.2588] [R loss: 0.1674 | F loss: 0.0254 | C loss: 0.1753 - C acc: 0.9219]\n",
      "1 - 7200/8297 \t [D loss: -0.6724] [G loss: -0.2207] [R loss: 0.2334 | F loss: 0.0164 | C loss: 0.2271 - C acc: 0.9141]\n",
      "1 - 7300/8297 \t [D loss: -0.6693] [G loss: -0.1844] [R loss: 0.2465 | F loss: 0.0316 | C loss: 0.2529 - C acc: 0.9062]\n",
      "1 - 7400/8297 \t [D loss: -0.6652] [G loss: -0.1754] [R loss: 0.1807 | F loss: 0.0291 | C loss: 0.1907 - C acc: 0.9141]\n",
      "1 - 7500/8297 \t [D loss: -0.6174] [G loss: -0.0961] [R loss: 0.1665 | F loss: 0.0425 | C loss: 0.1900 - C acc: 0.9453]\n",
      "1 - 7600/8297 \t [D loss: -0.6996] [G loss: -0.1114] [R loss: 0.2519 | F loss: 0.0538 | C loss: 0.2779 - C acc: 0.9062]\n",
      "1 - 7700/8297 \t [D loss: -0.6825] [G loss: -0.0721] [R loss: 0.4284 | F loss: 0.0475 | C loss: 0.4327 - C acc: 0.8906]\n",
      "1 - 7800/8297 \t [D loss: -0.6751] [G loss: -0.1378] [R loss: 0.2366 | F loss: 0.0339 | C loss: 0.2459 - C acc: 0.9141]\n",
      "1 - 7900/8297 \t [D loss: -0.5984] [G loss: -0.1584] [R loss: 0.1996 | F loss: 0.0408 | C loss: 0.2186 - C acc: 0.9375]\n",
      "1 - 8000/8297 \t [D loss: -0.6352] [G loss: -0.2169] [R loss: 0.1990 | F loss: 0.0414 | C loss: 0.2185 - C acc: 0.9375]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12492/3539832036.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m             )\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mgan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[0mgan\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_models\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfolder_path\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'saved_models'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12492/3271061880.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, epochs)\u001B[0m\n\u001B[0;32m    438\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    439\u001B[0m                         \u001B[1;31m# Train the critic.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 440\u001B[1;33m                         d_loss = self.critic_model.train_on_batch(\n\u001B[0m\u001B[0;32m    441\u001B[0m                             \u001B[1;33m[\u001B[0m\u001B[0mx_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_batch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnoise\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    442\u001B[0m                             [valid, fake, dummy])\n",
      "\u001B[1;32mD:\\lzj\\work\\code\\python\\machinelearning\\EC-GAN_NIDS-main\\.venv\\lib\\site-packages\\keras\\engine\\training_v1.py\u001B[0m in \u001B[0;36mtrain_on_batch\u001B[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001B[0m\n\u001B[0;32m   1074\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_sample_weight_modes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msample_weights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1075\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_train_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1076\u001B[1;33m       \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mins\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1077\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1078\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mreset_metrics\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\lzj\\work\\code\\python\\machinelearning\\EC-GAN_NIDS-main\\.venv\\lib\\site-packages\\keras\\backend.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   4184\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_callable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeed_arrays\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_symbols\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msymbol_vals\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4185\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4186\u001B[1;33m     fetched = self._callable_fn(*array_vals,\n\u001B[0m\u001B[0;32m   4187\u001B[0m                                 run_metadata=self.run_metadata)\n\u001B[0;32m   4188\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\lzj\\work\\code\\python\\machinelearning\\EC-GAN_NIDS-main\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1481\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1482\u001B[0m         \u001B[0mrun_metadata_ptr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_NewBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1483\u001B[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[0m\u001B[0;32m   1484\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1485\u001B[0m                                                run_metadata_ptr)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gan = ECGAN(x_train,\n",
    "            y_train,\n",
    "            num_classes=15,\n",
    "            latent_dim=32,\n",
    "            batch_size=128,\n",
    "            n_critic=5,\n",
    "            conf_thresh=.2,\n",
    "            adv_weight=.1\n",
    "            )\n",
    "\n",
    "gan.train(epochs=10)\n",
    "gan.save_models(folder_path='saved_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:51:12.924219900Z",
     "start_time": "2024-05-14T13:51:12.924219900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326.516px",
    "left": "920.969px",
    "right": "20px",
    "top": "81px",
    "width": "734.469px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
